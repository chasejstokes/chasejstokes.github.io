<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chase Stokes - Visualization Researcher</title>

    <link rel="icon" href="imgs/icon.png" type="image/x-icon">
    <link rel="shortcut icon" href="imgs/icon.png" type="image/x-icon">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">


    <!-- Custom CSS -->
    <link rel="stylesheet" href="main.css">
</head>

<body>
    <!-- navbar -->
    <nav id='mainNavbar' class="navbar navbar-expand-lg navbar-dark bf-dark">
        <h1 id="header">
            <a id="headerLink" href="index.html">Chase Stokes</a>
        </h1>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#mainNavCollapse">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id='mainNavCollapse'>
            <ul class="navbar-nav ms-auto">
                <li class="nav-item">
                    <a href="index.html" class="nav-link">About</a>
                </li>
                <li class="nav-item active">
                    <a href="research.html" class="nav-link">Research</a>
                </li>
                <!-- <li class="nav-item dropdown active">
                    <a id="navbarDropdown" class="nav-link dropdown-toggle" href="#" role="button"
                        data-toggle="dropdown">Research</a>
                    <ul class="dropdown-menu">
                        <li> <a class="dropdown-item active" href="research.html">All</a></li>
                        <li><a class="dropdown-item" href="research/lang_vis.html">Language and Visualization</a></li>
                        <li><a class="dropdown-item" href="research/percep_vis.html">Visualization Perception</a>
                        </li>
                    </ul>
                </li> -->
                <!-- <li class="nav-item">
                    <a href="teaching.html" class="nav-link">Teaching</a>
                </li> -->
                <li class="nav-item">
                    <a href="cv.html" class="nav-link">CV</a>
                </li>
                <li class="nav-item">
                    <a href="contact.html" class="nav-link">Contact</a>
                </li>
            </ul>
        </div>
    </nav>

    <div class="container-fluid">

        <div class="row information">
            <div class="col">
                <span class="title">2025</span>
            </div>
        </div>

        <!-- text functions -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/function_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>An Analysis of Text Functions in Information Visualization</b></br>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Anjana Arunkumar, Marti A.
                            Hearst, and Lace Padilla</i>
                    </p>
                    <p><i>IEEE Transations on Visualization and Computer Graphics, 2025.</i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#function-abstract" aria-expanded="false" aria-controls="function-abstract">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Text Functions in Visualization.pdf" target="_blank"
                            class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                            <a href="TODO_LINK" target="_blank"
                                class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="function-abstract" class="collapse mt-2">
                        <p class="mb-0">
                            Text is an integral but understudied component of visualization design. Although recent
                            studies have examined how text elements (e.g., titles and annotations) influence
                            comprehension, preferences, and predictions, many questions remain about textual design and
                            use in practice. This paper introduces a framework for understanding text functions in
                            information visualizations, building on and filling gaps in prior classifications and
                            taxonomies. Through an analysis of 120 real-world visualizations and 804 text elements, we
                            identified ten distinct text functions, ranging from identifying data mappings to presenting
                            valenced subtext. We further identify patterns in text usage and conduct a factor analysis,
                            revealing four overarching text-informed design strategies: Attribution and Variables,
                            Annotation-Centric Design, Visual Embellishments, and Narrative Framing. In addition to
                            these factors, we explore features of title rhetoric and text multifunctionality, while also
                            uncovering previously unexamined text functions, such as text replacing visual elements. Our
                            findings highlight the flexibility of text, demonstrating how different text elements in a
                            given design can combine to communicate, synthesize, and frame visual information. This
                            framework adds important nuance and detail to existing frameworks that analyze the diverse
                            roles of text in visualization. OSF materials can be found at <a
                                href="https://osf.io/swqfc/overview" target='_blank'>https://osf.io/swqfc/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- affordance -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/affordance_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances</b></br>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Kylie Lin, and Cindy Xiong
                            Bearfield</i>
                    </p>
                    <p><i>IEEE Transations on Visualization and Computer Graphics, 2025.</i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#affordance-abstract" aria-expanded="false"
                            aria-controls="affordance-abstract">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Methods for Studying Affordance.pdf" target="_blank"
                            class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                            <a href="TODO_LINK" target="_blank"
                                class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="affordance-abstract" class="collapse mt-2">
                        <p class="mb-0">
                            A growing body of work on visualization affordances highlights how specific design choices
                            shape reader takeaways from information visualizations. However, mapping the relationship
                            between design choices and reader conclusions often requires labor-intensive crowdsourced
                            studies, generating large corpora of free-response text for analysis. To address this
                            challenge, we explored alternative scalable research methodologies to assess chart
                            affordances. We test four elicitation methods from human-subject studies: free response,
                            visualization ranking, conclusion ranking, and salience rating, and compare their
                            effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps.
                            Overall, we find that while no method fully replicates affordances observed in free-response
                            conclusions, combinations of ranking and rating methods can serve as an effective proxy at a
                            broad scale. The two ranking methodologies were influenced by participant bias towards
                            certain chart types and the comparison of suggested conclusions. Rating conclusion salience
                            could not capture the specific variations between chart types observed in the other methods.
                            To supplement this work, we present a case study with GPT-4o, exploring the use of large
                            language models (LLMs) to elicit human-like chart interpretations. This aligns with recent
                            academic interest in leveraging LLMs as proxies for human participants to improve data
                            collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience
                            rating methodology but suffered from severe constraints in other areas. Overall, the
                            discrepancies in affordances we found between various elicitation methodologies, including
                            GPT-4o, highlight the importance of intentionally selecting and combining methods and
                            evaluating trade-offs. Materials for this work can be found at <a
                                href="https://osf.io/ynzhm/overview">https://osf.io/ynzhm/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- heal workshop paper -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/heal_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>LLMs Are Not Reliable Human Proxies to Study Affordances in Data Visualizations</b>
                    <p class='authors'><i> Kylie Lin, <span class="author-name">Chase Stokes</span>, and Cindy Xiong
                            Bearfield</i>
                    </p>
                    <p><i>Workshop on Human-Centered Evaluation and Auditing of Language Models at CHI
                            Conference on Human Factors in Computing Systems, 2025.
                        </i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#heal-abstract" aria-expanded="false" aria-controls="heal-abstract">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/LLMs as Human Proxy.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                            <a href="TODO_LINK" target="_blank"
                                class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="heal-abstract" class="collapse mt-2">
                        <p class="mb-0">
                            Identifying the relationship between data visualization design and human interpretation
                            often requires time-consuming crowdsourced studies that generate large text corpora.
                            Given recent academic exploration into the use of large language models (LLMs) as proxies
                            for human study participants, we anticipate interest within the visualization community on
                            LLM predictions as proxies for human chart interpretation. We present a case study on the
                            effectiveness of OpenAI's GPT-4o model to predict human takeaways from charts. Using the
                            lens of visualization affordances, we conduct a factor analysis on human chart takeaways,
                            identifying five affordance factors. We then compare the affordances of different chart
                            types between human readers and GPT-4o, revealing discrepancies in takeaway accuracy,
                            semantic diversity, response length, and alignment with human interpretations. We caution
                            against using LLMs as human proxies in empirical studies and outline critical directions for
                            future research on LLM predictions of human reasoning with data visualizations.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <div class="row information">
            <div class="col">
                <span class="title">2024</span>
            </div>
        </div>

        <!-- writing rudders -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/writingrudder_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>“It's a Good Idea to Put It Into Words”:
                        Writing 'Rudders' in the Initial Stages of Visualization Design</b>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Clara Hu, and Marti A.
                            Hearst</i>
                    </p>
                    <p><i>IEEE Transations on Visualization and Computer Graphics, 2024.</i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#writingrudder-abstract" aria-expanded="false"
                            aria-controls="writingrudder-abstract">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Writing 'Rudders'.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                            <a href="TODO_LINK" target="_blank"
                                class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="writingrudder-abstract" class="collapse mt-2">
                        <p class="mb-0">Written language is a useful tool for non-visual creative activities like
                            composing essays and planning searches. This
                            paper investigates the integration of written language into the visualization design
                            process. We create the idea of a 'writing rudder,'
                            which acts as a guiding force or strategy for the designer. Via an interview study of 24
                            working visualization designers, we first
                            established that only a minority of participants systematically use writing to aid in
                            design. A second study with 15 visualization designers
                            examined four different variants of written rudders: asking questions, stating conclusions,
                            composing a narrative, and writing titles.
                            Overall, participants had a positive reaction; designers recognized the benefits of
                            explicitly writing down components of the design and
                            indicated that they would use this approach in future design work. More specifically, two
                            approaches — writing questions and writing
                            conclusions/takeaways — were seen as beneficial across the design process, while writing
                            narratives showed promise mainly for the
                            creation stage. Although concerns around potential bias during data exploration were raised,
                            participants also discussed strategies to
                            mitigate such concerns. This paper contributes to a deeper understanding of the interplay
                            between language and visualization, and
                            proposes a straightforward, lightweight addition to the visualization design process.
                            OSF materials can be found at <a href=https://osf.io/yjsnh/overview
                                target="_blank_">https://osf.io/yjsnh/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- uncertainty workshop -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/voicinguncertainty_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>Voicing Uncertainty: How Speech, Text, and Visualizations Influence
                        Decisions with Data Uncertainty</b>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Chelsea Sanker, Bridget Cogley,
                            and Vidya
                            Setlur</i>
                    </p>
                    <p><i>Workshop on Uncertainty Visualization: Applications, Techniques, Software, and Decision
                            Frameworks
                            in conjunction with IEEE VIS Conference, 2024.
                        </i></p>


                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#voicinguncertainty-abstract" aria-expanded="false"
                            aria-controls="voicinguncertainty-abstract">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Voicing Uncertainty.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                            <a href="TODO_LINK" target="_blank"
                                class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="voicinguncertainty-abstract" class="collapse mt-2">
                        <p class="mb-0">Understanding and communicating data uncertainty is crucial for
                            informed decision-making across various domains, including finance, healthcare, and public
                            policy. This study investigates the
                            impact of gender and acoustic variables on decision-making, confidence, and trust through a
                            crowdsourced experiment. We compared
                            visualization-only representations of uncertainty to text-forward
                            and speech-forward bimodal representations, including multiple
                            synthetic voices across gender. Speech-forward representations led
                            to an increase in risky decisions, and text-forward representations
                            led to lower confidence. Contrary to prior work, speech-forward
                            forecasts did not receive higher ratings of trust. Higher normalized pitch led to a slight
                            increase in decision confidence, but other
                            voice characteristics had minimal impact on decisions and trust. An
                            exploratory analysis of accented speech showed consistent results
                            with the main experiment and additionally indicated lower trust ratings for information
                            presented in Indian and Kenyan accents. The
                            results underscore the importance of considering acoustic and contextual factors in
                            presentation of data uncertainty. Materials for this work can be found at <a
                                href='https://osf.io/mdz2y/overview' target='_blank'>https://osf.io/mdz2y/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>


        <!-- mixing modes -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/mixingmodes_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>Mixing Modes: Active and Passive Integration of Speech, Text, and
                        Visualization for Communicating Data Uncertainty</b>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Chelsea Sanker, Bridget Cogley,
                            and Vidya
                            Setlur</i>
                    </p>
                    <p><i>Proceedings of Eurographics Conference on Visualization in Computer Graphics Forum, 2024.</i>
                    </p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#mixingmodes-abstract" aria-expanded="false"
                            aria-controls="mixingmodes-abstract">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Mixing Modes.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <span class="mx-2">|</span>
                        <a href="https://www.tableau.com/research/publications/mixing-modes" target="_blank"
                            class="btn btn-link p-0">Watch Video</a>
                    </div>

                    <div id="mixingmodes-abstract" class="collapse mt-2">
                        <p class="mb-0">Interpreting uncertain data can be difficult, particularly if the data
                            presentation is complex. We investigate the efficacy of
                            different modalities for representing data and how to combine the strengths of each modality
                            to facilitate the communication
                            of data uncertainty. We implemented two multimodal prototypes to explore the design space of
                            integrating speech, text, and
                            visualization elements. A preliminary evaluation with 20 participants from academic and
                            industry communities demonstrates
                            that there exists no one-size-fits-all approach for uncertainty communication strategies;
                            rather, the effectiveness of conveying
                            uncertain data is intertwined with user preferences and situational context, necessitating a
                            more refined, multimodal strategy
                            for future interface design. Materials for this paper can be found at <a
                                href="https://osf.io/6g8ex/overview" target="_blank">https://osf.io/6g8ex/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- delays to densities -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/delaydensity_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>From Delays to Densities: Exploring Data Uncertainty through
                        Speech, Text, and Visualization</b>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Chelsea Sanker, Bridget Cogley,
                            and Vidya
                            Setlur</i>
                    </p>
                    <p><i>Proceedings of Eurographics Conference on Visualization in Computer Graphics Forum, 2024.</i>
                    </p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#delaydensity-description" aria-expanded="false"
                            aria-controls="delaydensity-description">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/From Delays to Densities.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <span class="mx-2">|</span>
                        <a href="https://www.tableau.com/research/publications/delays-and-densities" target="_blank"
                            class="btn btn-link p-0">Watch Video</a>
                    </div>

                    <div id="delaydensity-description" class="collapse mt-2">
                        <p class="mb-0">Understanding and communicating data uncertainty is crucial for making informed
                            decisions in sectors like finance and
                            healthcare. Previous work has explored how to express uncertainty in various modes. For
                            example, uncertainty can be expressed
                            visually with quantile dot plots or linguistically with hedge words and prosody. Our
                            research aims to systematically explore
                            how variations within each mode contribute to communicating uncertainty to the user; this
                            allows us to better understand
                            each mode's affordances and limitations. We completed an exploration of the uncertainty
                            design space based on pilot studies
                            and ran two crowdsourced experiments examining how speech, text, and visualization modes and
                            variants within them impact
                            decision-making with uncertain data. Visualization and text were most effective for rational
                            decision-making, though text
                            resulted in lower confidence. Speech garnered the highest trust despite sometimes leading to
                            risky decisions. Results from
                            these studies indicate meaningful trade-offs among modes of information and encourage
                            exploration of multimodal data
                            representations. Materials for this paper can be found at <a
                                href="https://osf.io/6g8ex/overview" target="_blank">https://osf.io/6g8ex/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>


        <!-- role of text -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/roletext_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>The Role of Text in Visualizations: How Annotations Shape
                        Perceptions of Bias and Influence Predictions</b> <a
                        href="http://www.replicabilitystamp.org#https-github-com-chasejstokes-role-text-git"><img
                            height="20px" src="https://www.replicabilitystamp.org/logo/Reproducibility-small.png"></a>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Cindy Bearfield Xiong, & Marti
                            A. Hearst</i>
                    </p>
                    <p><i>IEEE Transations on Visualization and Computer Graphics, 2024. </i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#roletext-description" aria-expanded="false"
                            aria-controls="roletext-description">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/The Role of Text in Visualizations.pdf" target="_blank"
                            class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                        <a href="TODO_LINK" target="_blank"
                            class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="roletext-description" class="collapse mt-2">
                        <p class="mb-0">This paper investigates the role of text in visualizations, specifically the
                            impact of text position, semantic content, and
                            biased wording. Two empirical studies were conducted based on two tasks (predicting data
                            trends and appraising bias) using two
                            visualization types (bar and line charts). While the addition of text had a minimal effect
                            on how people perceive data trends, there was
                            a significant impact on how biased they perceive the authors to be. This finding revealed a
                            relationship between the degree of bias in
                            textual information and the perception of the authors' bias. Exploratory analyses support an
                            interaction between a person's prediction
                            and the degree of bias they perceived. This paper also develops a crowdsourced method for
                            creating chart annotations that range from
                            neutral to highly biased. This research highlights the need for designers to mitigate
                            potential polarization of readers' opinions based on
                            how authors' ideas are expressed. Materials for this paper can be found at <a
                                href='https://osf.io/4bysj/overview' target='_blank'>https://osf.io/4bysj/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>



        <div class="row information">
            <div class="col">
                <span class="title">2023</span>
            </div>
        </div>

        <!-- bar chart groupings -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/groupingcues_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>What Does the Chart Say? Grouping Cues Guide Viewer
                        Comparisons and Conclusions in Bar Charts</b>
                    <p class='authors'><i>Cindy Bearfield Xiong, <span class="author-name">Chase Stokes</span>, Andrew
                            Lovett, & Steven
                            Franconeri</i>
                    </p>
                    <p><i>IEEE Transations on Visualization and Computer Graphics, 2023. </i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#groupingcues-description" aria-expanded="false"
                            aria-controls="groupingcues-description">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/What Does the Chart Say.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                        <a href="TODO_LINK" target="_blank"
                            class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="groupingcues-description" class="collapse mt-2">
                        <p class="mb-0">Reading a visualization is like reading a paragraph. Each sentence is a
                            comparison: the mean of these is higher than
                            those; this difference is smaller than that. What determines which comparisons are made
                            first? The viewer's goals and expertise
                            matter, but the way that values are visually grouped together within the chart also impacts
                            those comparisons. We create a visual
                            comparison taxonomy that allows us to develop and test a sequence of hypotheses about which
                            comparisons people are more likely
                            to make when reading a visualization. We find that people tend to compare two groups before
                            comparing two individual bars and that
                            second-order comparisons are rare. Visual cues like spatial proximity and color can
                            influence which elements are grouped together
                            and selected for comparison, with spatial proximity being a stronger grouping cue.
                            Interestingly, once the viewer grouped together
                            and compared a set of bars, regardless of whether the group is formed by spatial proximity
                            or color similarity, they no longer consider
                            other possible groupings in their comparisons.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="row information">
            <div class="col">
                <span class="title">2022</span>
            </div>
        </div>


        <!-- striking a balance -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/strikebalance_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>Striking a Balance: Reader Takeaways and Preferences when
                        Integrating Text and Charts</b>
                    <a href="http://www.replicabilitystamp.org#https-github-com-chasejstokes-balance-text-git"><img
                            height="20px" src="https://www.replicabilitystamp.org/logo/Reproducibility-small.png"></a>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span>, Vidya Setlur, Bridget Cogley,
                            Arvind
                            Satyanarayan, & Marti A. Hearst</i>
                    </p>
                    <p><i>IEEE Transations on Visualization and Computer Graphics, 2022. </i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#strikebalance-description" aria-expanded="false"
                            aria-controls="strikebalance-description">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Striking a Balance.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                    <a href="TODO_LINK" target="_blank"
                        class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="strikebalance-description" class="collapse mt-2">
                        <p class="mb-0">While visualizations are an effective way to represent insights about
                            information, they rarely stand alone. When designing
                            a visualization, text is often added to provide additional context and guidance for the
                            reader. However, there is little experimental
                            evidence to guide designers as to what is the right amount of text to show within a chart,
                            what its qualitative properties should be,
                            and where it should be placed. Prior work also shows variation in personal preferences for
                            charts versus textual representations. In
                            this paper, we explore several research questions about the relative value of textual
                            components of visualizations. We found that participants
                            preferred the charts with the largest number of
                            textual annotations over charts with fewer annotations or text alone. We also found effects
                            of semantic content. For instance, the text that
                            describes statistical or relational components of a chart leads to more takeaways of a
                            similar nature than text describing elemental
                            or encoded components. Finally, we found different effects for the semantic levels based on
                            the placement of the text on the chart;
                            some kinds of information are best placed in the title, while others should be placed closer
                            to the data. These results have important
                            implications for chart design guidelines and future work pertaining to the combination of
                            text and charts. Materials for this paper can be found at <a
                                href='https://osf.io/vz976/overview' target="_blank">https://osf.io/vz976/overview</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- more text often better -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/moretext_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>Why More Text is (Often) Better:
                        Themes from Reader Preferences for Integration of Charts and Text</b>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span> & Marti A. Hearst</i>
                    </p>
                    <p><i>Workshop on NLVIZ: Exploring Research Opportunities for Natural Language, Text, and Data
                            Visualization
                            in conjunction with IEEE VIS Conference, 2022.
                        </i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#moretext-description" aria-expanded="false"
                            aria-controls="moretext-description">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Why More Text is (Often) Better.pdf" target="_blank"
                            class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                <a href="TODO_LINK" target="_blank"
                    class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="moretext-description" class="collapse mt-2">
                        <p class="mb-0">Given a choice between charts with minimal and those with copious
                            textual annotations, participants in a study (<a href="pdfs/Striking a Balance.pdf"
                                target="_blank">Stokes et al., 2022</a>) tended to prefer the
                            charts with more text. This paper examines the qualitative responses
                            of the participants' preferences for various stimuli integrating charts
                            and text, including a text-only variant. A thematic analysis of these
                            responses resulted in three main findings. First, readers commented
                            most frequently on the presence or lack of context or detail; they
                            preferred to be informed, even at the cost of simplicity. Second,
                            readers discussed the story-like component of the text-only variant,
                            making little mention of narrative in relation to the chart variants.
                            Finally, readers showed suspicion around possible misleading elements of the chart or text.
                            These themes support findings from
                            previous work on annotations, captions, and alternative text and
                            raise further questions in the study of the combination of text and
                            visual communication. Materials for this paper can be found at <a
                                href='https://osf.io/nwtsf/overview' target="_blank">https://osf.io/nwtsf/overview</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <!-- belief bias -->
        <div class="row information mb-4">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/seebelieve_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>Seeing What You Believe or Believing What You See?
                        Belief Biases Correlation Estimation</b>
                    <p class='authors'><i>Cindy Xiong Bearfield, <span class="author-name">Chase Stokes</span>, Yea-Seul
                            Kim, Steven
                            Franconeri</i>
                    </p>
                    <p><i>IEEE Transations on Visualization and Computer Graphics, 2022. </i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#seebelieve-description" aria-expanded="false"
                            aria-controls="seebelieve-description">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Seeing What You Believe or Believing What You See.pdf" target="_blank"
                            class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                    <a href="TODO_LINK" target="_blank"
                        class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="seebelieve-description" class="collapse mt-2">
                        <p class="mb-0">When an analyst or scientist has a belief about how the world works, their
                            thinking can be biased in favor of that belief.
                            Therefore, one bedrock principle of science is to minimize that bias by testing the
                            predictions of one's belief against objective data. Through two crowdsourced experiments, we
                            demonstrate
                            that supposedly objective assessments of the strength of a correlational relationship can be
                            influenced by how strongly a viewer
                            believes in the existence of that relationship. Participants viewed in scatterplots
                            depicting a relationship between variable
                            pairs and estimated their correlations. After completing a distractor task,
                            they also reported how strongly they believed there to be a correlation between the depicted
                            variable pairs. In a control condition, they
                            judged the same scatterplots labeled instead with generic 'X' and 'Y' axes. Participants
                            estimated correlations more accurately when
                            they viewed scatterplots labeled with generic axes compared to scatterplots labeled with
                            meaningful variable pairs. Furthermore,
                            when viewers believed that two variables should have a strong relationship, they
                            overestimated correlations between those variables
                            by around an r-value of 0.1. When they believed that the variables should be unrelated, they
                            underestimated the correlations by
                            around an r-value of 0.1. While data visualizations are typically thought to present
                            objective truths to the viewer, these results suggest
                            that existing personal beliefs can bias even objective statistical values people extract
                            from data. Materials for this paper can be found at <a href='https://osf.io/3cnza/overview'
                                target="_blank">https://osf.io/3cnza/overview</a>.</p>
                    </div>
                </div>
            </div>
        </div>



        <div class="row information">
            <div class="col">
                <span class="title">2021</span>
            </div>
        </div>

        <!-- give text a chance -->
        <div class="row information mb-4 last-item">
            <div class="col-lg-11 col-sm-12 d-flex align-items-start">
                <img src="imgs/givetext_teaser_img.png" alt="" class="teaser me-3">

                <div>
                    <b>Give Text A Chance:
                        Advocating for Equal Consideration for Language and Visualization</b>
                    <p class='authors'><i><span class="author-name">Chase Stokes</span> & Marti A. Hearst</i>
                    </p>
                    <p><i>Workshop on NLVIZ: Exploring Research Opportunities for Natural Language, Text, and Data
                            Visualization
                            in conjunction with IEEE VIS Conference, 2021.
                        </i></p>

                    <div class="d-flex align-items-center">
                        <button class="btn btn-sm btn-outline-secondary me-3" type="button" data-bs-toggle="collapse"
                            data-bs-target="#givetext-description" aria-expanded="false"
                            aria-controls="givetext-description">
                            Read Abstract
                        </button>
                        <span class="mx-2">|</span>
                        <a href="pdfs/Give Text A Chance.pdf" target="_blank" class="btn btn-link p-0 me-3">Read
                            Paper</a>
                        <!-- TODO: CREATE AND ADD VIDEO LINK -->
                        <!-- <span class="mx-2">|</span>
                    <a href="TODO_LINK" target="_blank"
                        class="btn btn-link p-0">Watch Video</a> -->
                    </div>

                    <div id="givetext-description" class="collapse mt-2">
                        <p class="mb-0">Visualization research tends to de-emphasize consideration of the
                            textual context in which its images are placed. We argue that visualization research should
                            consider textual representations as a primary
                            alternative to visual options when assessing designs, and when assessing designs, equal
                            attention should be given to the construction
                            of the language as to the visualizations. We also call for a consideration of readability
                            when integrating visualizations with written text.
                            In highlighting these points, visualization research would be elevated
                            in efficacy and demonstrate thorough accounting for viewers' needs
                            and responses.</p>
                    </div>
                </div>
            </div>
        </div>


    </div>

    <!-- footer -->
    <footer class="footer mt-auto">
        <div class="container">
            <ul class="nav justify-content-center list-unstyled">
                <li class='footer-item'>
                    <a href="https://twitter.com/chasejstokes" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" class="bi bi-twitter"
                            viewBox="0 0 16 16">
                            <path
                                d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z" />
                        </svg>
                    </a>
                </li>
                <!-- <li class='footer-item'>
                    <a href="https://vis.social/@chase" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" class="bi bi-mastodon"
                            viewBox="0 0 16 16">
                            <path
                                d="M11.19 12.195c2.016-.24 3.77-1.475 3.99-2.603.348-1.778.32-4.339.32-4.339 0-3.47-2.286-4.488-2.286-4.488C12.062.238 10.083.017 8.027 0h-.05C5.92.017 3.942.238 2.79.765c0 0-2.285 1.017-2.285 4.488l-.002.662c-.004.64-.007 1.35.011 2.091.083 3.394.626 6.74 3.78 7.57 1.454.383 2.703.463 3.709.408 1.823-.1 2.847-.647 2.847-.647l-.06-1.317s-1.303.41-2.767.36c-1.45-.05-2.98-.156-3.215-1.928a3.614 3.614 0 0 1-.033-.496s1.424.346 3.228.428c1.103.05 2.137-.064 3.188-.189zm1.613-2.47H11.13v-4.08c0-.859-.364-1.295-1.091-1.295-.804 0-1.207.517-1.207 1.541v2.233H7.168V5.89c0-1.024-.403-1.541-1.207-1.541-.727 0-1.091.436-1.091 1.296v4.079span.197V5.522c0-.859.22-1.541.66-2.046.456-.505 1.052-.764 1.793-.764.856 0 1.504.328 1.933.983L8 4.39l.417-.695c.429-.655 1.077-.983 1.934-.983.74 0 1.336.259 1.791.764.442.505.661 1.187.661 2.046v4.203z" />
                        </svg>
                    </a>
                </li> -->
                <li class='footer-item'>
                    <a href="https://scholar.google.com/citations?user=ZJ7ydGcAAAAJ&hl=en&oi=ao" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" class="bi bi-mortarboard-fill"
                            viewBox="0 0 16 16">
                            <path
                                d="M8.211 2.047a.5.5 0 0 0-.422 0l-7.5 3.5a.5.5 0 0 0 .025.917l7.5 3a.5.5 0 0 0 .372 0L14 7.14V13a1 1 0 0 0-1 1v2h3v-2a1 1 0 0 0-1-1V6.739l.686-.275a.5.5 0 0 0 .025-.917l-7.5-3.5Z" />
                            <path
                                d="M4.176 9.032a.5.5 0 0 0-.656.327l-.5 1.7a.5.5 0 0 0 .294.605l4.5 1.8a.5.5 0 0 0 .372 0l4.5-1.8a.5.5 0 0 0 .294-.605l-.5-1.7a.5.5 0 0 0-.656-.327L8 10.466 4.176 9.032Z" />
                        </svg>
                    </a>
                </li>
                <li class='footer-item'>
                    <a href="https://www.linkedin.com/in/chasejstokes" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" class="bi bi-linkedin"
                            viewBox="0 0 16 16">
                            <path
                                d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z" />
                        </svg>
                    </a>
                </li>
            </ul>
        </div>
    </footer>


    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.3/dist/umd/popper.min.js"
        integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js"
        integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
        crossorigin="anonymous"></script>
    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

</body>

</html>